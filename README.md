# AGI-Alignment-and-Safety-Research
Investigating AGI alignment, safety, and ethical considerations in AI systems through causal reasoning, red-teaming prompts, and fairness evaluations across large language models (LLMs).
